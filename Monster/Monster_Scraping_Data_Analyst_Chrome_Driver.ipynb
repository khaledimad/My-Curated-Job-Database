{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os \n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/32 [00:00<00:07,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 2/32 [00:00<00:07,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 3/32 [00:00<00:07,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 4/32 [00:01<00:07,  3.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7/32 [00:02<00:09,  2.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 8/32 [00:03<00:10,  2.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 9/32 [00:03<00:10,  2.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 10/32 [00:04<00:11,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 11/32 [00:04<00:08,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12/32 [00:04<00:07,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13/32 [00:04<00:05,  3.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14/32 [00:04<00:05,  3.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15/32 [00:05<00:04,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 16/32 [00:05<00:04,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17/32 [00:05<00:03,  4.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 18/32 [00:05<00:03,  4.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 19/32 [00:06<00:03,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 20/32 [00:06<00:03,  3.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21/32 [00:06<00:02,  4.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22/32 [00:06<00:02,  4.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23/32 [00:07<00:02,  4.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 24/32 [00:07<00:01,  4.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25/32 [00:07<00:01,  4.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 26/32 [00:07<00:01,  4.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 27/32 [00:07<00:01,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28/32 [00:08<00:00,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29/32 [00:08<00:00,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30/32 [00:08<00:00,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31/32 [00:08<00:00,  4.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.50it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Seattle, WA\n",
    "links1 = []\n",
    "for i in tqdm(range(32)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Seattle__2c-WA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:06,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:07,  3.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:07,  3.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:07,  3.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:08,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:02<00:09,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:02<00:09,  2.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:03<00:10,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:03<00:10,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:04<00:10,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:04<00:08,  2.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:04<00:06,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:05<00:06,  2.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:05<00:04,  3.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:04,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:05<00:03,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:06<00:03,  4.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:06<00:02,  4.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:06<00:02,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:06<00:02,  4.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:06<00:02,  4.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:07<00:01,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:07<00:01,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:07<00:01,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:07<00:01,  4.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:08<00:00,  4.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:08<00:00,  4.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:08<00:00,  4.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:08<00:00,  4.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:08<00:00,  3.36it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# New York, NY 1\n",
    "links2_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=New+York__2c-NY&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links2_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:07,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:06,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:06,  4.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:05,  4.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:05,  4.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:01<00:05,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:01<00:05,  4.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:01<00:05,  4.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:02<00:04,  4.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:02<00:04,  4.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:02<00:04,  4.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:02<00:04,  4.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:03<00:04,  4.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:03<00:03,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:03<00:03,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:03<00:03,  3.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:04<00:03,  3.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:04<00:03,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:04<00:02,  3.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:04<00:02,  3.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:05<00:02,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:05<00:02,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:05<00:01,  3.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:06<00:01,  3.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:06<00:01,  3.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:06<00:01,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:06<00:00,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:07<00:00,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:07<00:00,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:07<00:00,  3.96it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# New York, NY 2\n",
    "links2_2 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=New+York__2c-NY&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links2_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 1/15 [00:00<00:05,  2.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 2/15 [00:00<00:04,  3.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 3/15 [00:00<00:03,  3.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 4/15 [00:01<00:03,  3.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 5/15 [00:01<00:03,  3.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 6/15 [00:01<00:02,  3.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 7/15 [00:02<00:02,  3.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 8/15 [00:02<00:01,  3.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 9/15 [00:02<00:01,  4.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 10/15 [00:02<00:01,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 11/15 [00:06<00:04,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 12/15 [00:08<00:05,  1.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 13/15 [00:11<00:03,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 14/15 [00:14<00:02,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.13s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# New York, NY 3\n",
    "links2_3 = []\n",
    "for i in tqdm(range(15)):\n",
    "    page = i + 60\n",
    "    page2 = i + 61\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=New+York__2c-NY&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links2_3.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:06,  4.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:06,  4.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:07,  3.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:07,  3.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:08,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:02<00:09,  2.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:02<00:09,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:03<00:10,  2.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:03<00:09,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:04<00:10,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:04<00:08,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:04<00:06,  2.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:05<00:05,  3.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:05<00:04,  3.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:03,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:05<00:03,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:05<00:03,  4.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:06<00:02,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:06<00:02,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:06<00:02,  4.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:06<00:01,  4.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:07<00:01,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:07<00:01,  3.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:07<00:01,  4.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:07<00:01,  4.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:07<00:00,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:08<00:00,  4.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:08<00:00,  4.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:08<00:00,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:08<00:00,  3.42it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Boston, MA 1\n",
    "links3_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=boston__2c-MA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links3_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:06,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:06,  4.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:06,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:05,  4.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:05,  4.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:01<00:05,  4.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:01<00:04,  4.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:01<00:04,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:01<00:04,  4.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:02<00:04,  4.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:02<00:04,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:02<00:03,  4.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:02<00:03,  4.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:03<00:03,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:13,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:08<00:20,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:12<00:28,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:15<00:28,  2.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:19<00:31,  2.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:23<00:32,  3.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:27<00:31,  3.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:33<00:33,  4.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:39<00:33,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:44<00:28,  4.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:48<00:23,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:54<00:19,  4.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [01:00<00:16,  5.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [01:06<00:10,  5.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [01:12<00:05,  5.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [01:18<00:00,  2.61s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Boston, MA 2\n",
    "links3_2 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=boston__2c-MA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links3_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/17 [00:06<01:50,  6.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 2/17 [00:11<01:34,  6.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 3/17 [00:15<01:17,  5.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 4/17 [00:21<01:14,  5.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 5/17 [00:27<01:08,  5.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 6/17 [00:33<01:04,  5.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 7/17 [00:39<00:58,  5.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 8/17 [00:47<00:59,  6.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 9/17 [00:54<00:51,  6.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 10/17 [01:00<00:44,  6.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 11/17 [01:06<00:38,  6.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 12/17 [01:11<00:30,  6.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 13/17 [01:18<00:24,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 14/17 [01:23<00:18,  6.01s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 15/17 [01:29<00:12,  6.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 16/17 [01:36<00:06,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [01:42<00:00,  6.01s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Boston, MA 3\n",
    "links3_3 = []\n",
    "for i in tqdm(range(17)):\n",
    "    page = i + 60\n",
    "    page2 = i + 61\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=boston__2c-MA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links3_3.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 1/13 [00:00<00:03,  4.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:00<00:03,  3.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:00<00:02,  3.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:01<00:03,  2.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:02<00:03,  2.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:02<00:03,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:03<00:03,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:04<00:03,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:05<00:03,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [00:05<00:02,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [00:06<00:01,  1.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [00:06<00:00,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Raleigh, NC\n",
    "links4 = []\n",
    "for i in tqdm(range(13)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Raleigh__2c-NC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Use for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links4.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/38 [00:00<00:08,  4.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 2/38 [00:00<00:08,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 3/38 [00:00<00:09,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 4/38 [00:01<00:09,  3.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 5/38 [00:01<00:10,  3.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 6/38 [00:02<00:11,  2.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 7/38 [00:02<00:12,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 8/38 [00:03<00:13,  2.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 9/38 [00:03<00:13,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 10/38 [00:04<00:14,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 11/38 [00:04<00:12,  2.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 12/38 [00:04<00:09,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 13/38 [00:04<00:07,  3.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 14/38 [00:05<00:06,  3.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 15/38 [00:05<00:06,  3.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 16/38 [00:05<00:05,  3.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 17/38 [00:05<00:05,  3.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 18/38 [00:06<00:04,  4.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 19/38 [00:06<00:04,  4.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 20/38 [00:06<00:04,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 21/38 [00:06<00:04,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 22/38 [00:06<00:03,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 23/38 [00:07<00:03,  3.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 24/38 [00:07<00:03,  4.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 25/38 [00:07<00:03,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 26/38 [00:07<00:02,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 27/38 [00:08<00:02,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 28/38 [00:08<00:02,  4.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 29/38 [00:08<00:01,  4.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 30/38 [00:08<00:01,  4.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 31/38 [00:08<00:01,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 32/38 [00:09<00:01,  4.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 33/38 [00:09<00:01,  4.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 34/38 [00:09<00:00,  4.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 35/38 [00:09<00:00,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 36/38 [00:10<00:00,  3.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 37/38 [00:10<00:00,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 38/38 [00:10<00:00,  3.57it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Chicago, IL\n",
    "links5 = []\n",
    "for i in tqdm(range(38)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Chicago__2c-IL&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links5.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:06,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:06,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:07,  3.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:07,  3.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:08,  2.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:02<00:08,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:02<00:08,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:03<00:10,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:03<00:10,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:04<00:13,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:05<00:10,  1.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:05<00:08,  2.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:05<00:06,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:05<00:05,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:04,  3.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:06<00:03,  3.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:06<00:03,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:06<00:03,  3.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:06<00:02,  3.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:07<00:02,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:07<00:02,  3.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:07<00:02,  3.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:08<00:01,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:08<00:01,  3.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:08<00:01,  3.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:08<00:01,  3.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:09<00:00,  3.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:09<00:00,  3.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:09<00:00,  3.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.01it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Washington, DC 1\n",
    "links6_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Washington__2c-DC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links6_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:05,  5.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:05,  4.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:05,  4.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:06,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:06,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:01<00:05,  4.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:01<00:05,  4.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:01<00:05,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:02<00:04,  4.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:02<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:02<00:04,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:02<00:03,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:03<00:03,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:03<00:03,  4.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:03<00:03,  4.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:03<00:03,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:03<00:02,  4.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:04<00:02,  4.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:04<00:02,  4.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:04<00:02,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:04<00:01,  4.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:05<00:01,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:05<00:01,  4.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:05<00:01,  4.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:05<00:01,  4.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:05<00:00,  4.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:06<00:00,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:06<00:00,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:06<00:00,  3.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.30it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Washington, DC\n",
    "links6_2 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Washington__2c-DC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links6_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/17 [00:00<00:03,  4.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 2/17 [00:00<00:03,  4.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 3/17 [00:00<00:02,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 4/17 [00:00<00:02,  4.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 5/17 [00:01<00:02,  4.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 6/17 [00:01<00:02,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 7/17 [00:01<00:02,  4.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 8/17 [00:01<00:02,  4.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 9/17 [00:02<00:01,  4.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 10/17 [00:02<00:01,  3.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 11/17 [00:02<00:01,  3.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 12/17 [00:03<00:01,  3.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 13/17 [00:03<00:01,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 14/17 [00:03<00:00,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 15/17 [00:03<00:00,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 16/17 [00:03<00:00,  4.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 17/17 [00:04<00:00,  4.19it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Washington, DC\n",
    "links6_3 = []\n",
    "for i in tqdm(range(17)):\n",
    "    page = i + 60\n",
    "    page2 = i + 61\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Washington__2c-DC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links6_3.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 1/12 [00:00<00:02,  3.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 2/12 [00:00<00:02,  3.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 4/12 [00:01<00:02,  3.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 5/12 [00:01<00:02,  3.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 6/12 [00:01<00:02,  2.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 7/12 [00:02<00:01,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 8/12 [00:02<00:01,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 9/12 [00:03<00:01,  2.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:04<00:01,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 11/12 [00:04<00:00,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 12/12 [00:04<00:00,  2.63it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Austin, TX\n",
    "links7 = []\n",
    "for i in tqdm(range(12)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Austin__2c-TX&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links7.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/32 [00:00<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 2/32 [00:00<00:07,  4.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 3/32 [00:00<00:07,  3.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 4/32 [00:01<00:09,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5/32 [00:01<00:10,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7/32 [00:02<00:11,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 8/32 [00:03<00:11,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 9/32 [00:03<00:11,  1.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 10/32 [00:04<00:13,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 11/32 [00:05<00:10,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12/32 [00:05<00:09,  2.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13/32 [00:05<00:07,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15/32 [00:06<00:05,  3.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 16/32 [00:06<00:04,  3.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17/32 [00:06<00:04,  3.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 18/32 [00:06<00:03,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 19/32 [00:07<00:03,  3.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 20/32 [00:07<00:03,  3.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21/32 [00:07<00:02,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22/32 [00:07<00:02,  4.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23/32 [00:08<00:02,  4.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 24/32 [00:08<00:02,  3.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25/32 [00:08<00:01,  3.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 26/32 [00:09<00:01,  3.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 27/32 [00:09<00:01,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28/32 [00:09<00:01,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29/32 [00:09<00:00,  3.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30/32 [00:10<00:00,  3.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31/32 [00:10<00:00,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.01it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Atlanta, GA\n",
    "links8 = []\n",
    "for i in tqdm(range(32)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Atlanta__2c-GA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links8.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:05,  5.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:06,  4.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:06,  3.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:07,  3.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:08,  2.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:02<00:09,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:02<00:09,  2.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:03<00:10,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:03<00:10,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:04<00:11,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:04<00:09,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:05<00:07,  2.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:05<00:05,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:05<00:05,  3.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:04,  3.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:05<00:03,  3.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:06<00:03,  4.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:06<00:02,  4.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:06<00:02,  4.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:06<00:02,  4.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:07<00:02,  3.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:07<00:02,  3.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:07<00:01,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:07<00:01,  4.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:08<00:01,  4.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:08<00:00,  4.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:08<00:00,  4.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:08<00:00,  4.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:09<00:00,  3.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.21it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Los Angeles, CA 1\n",
    "links9_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Los+Angeles__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links9_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1/20 [00:00<00:03,  5.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 2/20 [00:00<00:03,  5.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 3/20 [00:00<00:04,  3.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 4/20 [00:01<00:03,  4.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 5/20 [00:01<00:03,  4.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 6/20 [00:01<00:03,  4.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 7/20 [00:01<00:03,  4.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 8/20 [00:01<00:02,  4.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 9/20 [00:02<00:02,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 10/20 [00:02<00:02,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 11/20 [00:02<00:02,  4.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 12/20 [00:04<00:06,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 13/20 [00:07<00:09,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 14/20 [00:11<00:12,  2.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 15/20 [00:14<00:12,  2.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 16/20 [00:18<00:11,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 17/20 [00:23<00:10,  3.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 18/20 [00:27<00:07,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 19/20 [00:30<00:03,  3.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 20/20 [00:34<00:00,  1.72s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Los Angeles, CA 2\n",
    "links9_2 = []\n",
    "for i in tqdm(range(20)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Los+Angeles__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links9_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 1/11 [00:00<00:02,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 2/11 [00:00<00:02,  3.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 3/11 [00:00<00:02,  3.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 4/11 [00:01<00:02,  3.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 5/11 [00:01<00:02,  2.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 6/11 [00:02<00:02,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 7/11 [00:02<00:01,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 8/11 [00:03<00:01,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 9/11 [00:03<00:01,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 10/11 [00:04<00:00,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 11/11 [00:04<00:00,  2.33it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# San Diego, CA\n",
    "links10 = []\n",
    "for i in tqdm(range(11)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=30&where=San+Diego__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links10.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:12,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:10,  2.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:01<00:10,  2.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:01<00:09,  2.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:09,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:02<00:10,  2.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:02<00:10,  2.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:03<00:09,  2.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:03<00:10,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:04<00:10,  1.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:04<00:09,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:05<00:07,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:05<00:05,  2.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:05<00:04,  3.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:05<00:04,  3.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:05<00:03,  3.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:06<00:03,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:06<00:02,  4.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:06<00:02,  4.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:06<00:02,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:06<00:01,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:07<00:01,  4.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:07<00:01,  4.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:07<00:01,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:07<00:01,  4.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:08<00:01,  3.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:08<00:00,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:08<00:00,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:09<00:00,  3.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.19it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# San Francisco, CA\n",
    "links11_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=50&where=San+Francisco__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links11_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/30 [00:00<00:06,  4.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:07,  3.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:06,  4.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:06,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5/30 [00:01<00:05,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6/30 [00:01<00:05,  4.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7/30 [00:01<00:05,  3.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8/30 [00:01<00:05,  4.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9/30 [00:02<00:05,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11/30 [00:02<00:04,  4.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12/30 [00:02<00:04,  4.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13/30 [00:03<00:03,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14/30 [00:03<00:03,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15/30 [00:03<00:03,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:03<00:03,  4.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17/30 [00:03<00:02,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 18/30 [00:04<00:02,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 19/30 [00:04<00:02,  4.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:04<00:02,  4.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 21/30 [00:04<00:02,  4.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:05<00:01,  4.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 23/30 [00:05<00:01,  4.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 24/30 [00:05<00:01,  4.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:05<00:01,  4.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:05<00:00,  4.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 27/30 [00:06<00:00,  4.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:08<00:01,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:10<00:01,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 30/30 [00:13<00:00,  2.27it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# San Francisco, CA\n",
    "links11_2 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=50&where=San+Francisco__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links11_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 1/7 [00:00<00:01,  3.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▊       | 2/7 [00:00<00:01,  3.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  4.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████▏  | 5/7 [00:01<00:00,  5.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  5.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.12it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# San Francisco, CA\n",
    "links11_3 = []\n",
    "for i in tqdm(range(7)):\n",
    "    page = i + 60\n",
    "    page2 = i + 61\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Analyst&intcid=skr_navigation_nhpso_searchMain&rad=50&where=San+Francisco__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links11_3.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links1+links2_1+links2_2+links2_3+links3_1+links3_2+links3_3+links4+links5+links6_1+links6_2+links6_3+links7+links8+links9_1+links9_2+links10+links11_1+links11_2+links11_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original list 23703\n"
     ]
    }
   ],
   "source": [
    "list_links = links\n",
    "print(\"Length of original list\",len(list_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of distinct list: 8737\n"
     ]
    }
   ],
   "source": [
    "# Eliminate repeated links\n",
    "distinct_links = (Counter(list_links).keys())\n",
    "distinct_links = [*distinct_links]\n",
    "print(\"Length of distinct list:\",len(distinct_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/774 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/774 [00:07<1:40:47,  7.82s/it]\u001b[A\n",
      "  0%|          | 2/774 [00:14<1:35:56,  7.46s/it]\u001b[A\n",
      "  0%|          | 3/774 [00:20<1:32:12,  7.18s/it]\u001b[A\n",
      "  1%|          | 4/774 [00:27<1:29:07,  6.94s/it]\u001b[A\n",
      "  1%|          | 5/774 [00:33<1:27:00,  6.79s/it]\u001b[A"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=80.0.3987.132)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f90c0d6d83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'profile'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.htm'\u001b[0m \u001b[0;31m#The new file name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=80.0.3987.132)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "d = webdriver.Chrome(executable_path=os.path.abspath('chromedriver'))   \n",
    "for i in tqdm(range(len(distinct_links))):\n",
    "    time.sleep(1)\n",
    "    num=str(i)\n",
    "    url = distinct_links[i]\n",
    "    \n",
    "    body = d.find_element_by_tag_name(\"body\")\n",
    "    body.send_keys(Keys.CONTROL + 't')\n",
    "    \n",
    "    d.get(url)\n",
    "    d.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 'w') \n",
    "    \n",
    "    time.sleep(4)\n",
    "    path = os.getcwd() + \"/Profiles/\"\n",
    "    name='profile'+num+'.htm' #The new file name. \n",
    "    with open(path + name, 'w', encoding='utf-8') as file:\n",
    "        file.write(d.page_source)\n",
    "        file.close()\n",
    "\n",
    "#time.sleep(4)\n",
    "#Close the google webpage that webdriver open for you, otherwise it will be crazy.\n",
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
