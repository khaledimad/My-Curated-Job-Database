{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os \n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:11<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Seattle, WA\n",
    "links1 = []\n",
    "for i in tqdm(range(38)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Seattle__2c-WA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# New York, NY 1\n",
    "links2_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=New+York__2c-NY&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links2_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# New York, NY 2\n",
    "links2_2 = []\n",
    "for i in tqdm(range(20)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=New+York__2c-NY&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links2_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:08<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Boston, MA\n",
    "links3 = []\n",
    "for i in tqdm(range(32)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=boston__2c-MA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links3.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Raleigh, NC\n",
    "links4 = []\n",
    "for i in tqdm(range(14)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Raleigh__2c-NC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links4.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:06<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Chicago, IL\n",
    "links5 = []\n",
    "for i in tqdm(range(22)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Chicago__2c-IL&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links5.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Washington, DC 1\n",
    "links6_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Washington__2c-DC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links6_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Washington, DC\n",
    "links6_2 = []\n",
    "for i in tqdm(range(20)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Washington__2c-DC&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links6_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Austin, TX\n",
    "links7 = []\n",
    "for i in tqdm(range(11)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Austin__2c-TX&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links7.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:05<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Atlanta, GA\n",
    "links8 = []\n",
    "for i in tqdm(range(17)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Atlanta__2c-GA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links8.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:07<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Los Angeles, CA\n",
    "links9 = []\n",
    "for i in tqdm(range(26)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=Los+Angeles__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links9.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# San Diego, CA\n",
    "links10 = []\n",
    "for i in tqdm(range(10)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=30&where=San+Diego__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links10.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# San Francisco, CA\n",
    "links11_1 = []\n",
    "for i in tqdm(range(30)):\n",
    "    page = i\n",
    "    page2 = i + 1\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=50&where=San+Francisco__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links11_1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# San Francisco, CA\n",
    "links11_2 = []\n",
    "for i in tqdm(range(25)):\n",
    "    page = i + 30\n",
    "    page2 = i + 31\n",
    "    url = \"https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&intcid=skr_navigation_nhpso_searchMain&rad=50&where=San+Francisco__2c-CA&stpage=\"+str(page)+\"&page=\"+str(page2)    \n",
    "    user_agent = 'Mozilla/5.0'\n",
    "    # Adding the user agent to the request header\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    #specifying a desired format of “page” using the html parser - \n",
    "    #this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    data = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # get all a tags with the given class\n",
    "    total_links = data.findAll(\"h2\", class_=\"title\")\n",
    "    \n",
    "    # Usee for loop to obtain urls \n",
    "    for i in range(len(total_links)):\n",
    "        link = total_links[i].find(\"a\",href=True)['href']\n",
    "        links11_2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links1+links2_1+links2_2+links3+links4+links5+links6_1+links6_2+links7+links8+links9+links10+links11_1+links11_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original list 20765\n"
     ]
    }
   ],
   "source": [
    "list_links = links\n",
    "print(\"Length of original list\",len(list_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of distinct list: 6775\n"
     ]
    }
   ],
   "source": [
    "# Eliminate repeated links\n",
    "distinct_links = (Counter(list_links).keys())\n",
    "distinct_links = [*distinct_links]\n",
    "print(\"Length of distinct list:\",len(distinct_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6775/6775 [8:44:50<00:00,  4.65s/it]   \n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "d = webdriver.Chrome(executable_path=os.path.abspath('chromedriver'))   \n",
    "for i in tqdm(range(len(distinct_links))):\n",
    "    time.sleep(1)\n",
    "    num=str(i)\n",
    "    url = distinct_links[i]\n",
    "    \n",
    "    body = d.find_element_by_tag_name(\"body\")\n",
    "    body.send_keys(Keys.CONTROL + 't')\n",
    "    \n",
    "    d.get(url)\n",
    "    d.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 'w') \n",
    "    \n",
    "    time.sleep(2)\n",
    "    path = os.getcwd() + \"/Profiles/\"\n",
    "    name='profile'+num+'.htm' #The new file name. \n",
    "    with open(path + name, 'w', encoding='utf-8') as file:\n",
    "        file.write(d.page_source)\n",
    "        file.close()\n",
    "\n",
    "#time.sleep(4)\n",
    "#Close the google webpage that webdriver open for you, otherwise it will be crazy.\n",
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
